{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 12: ML Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this homework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/data_utils.py\n",
        "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/image_utils.py\n",
        "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/lfw.tar.gz | tar xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import randint\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from data_utils import LFWUtils\n",
        "from data_utils import classification_error, display_confusion_matrix\n",
        "from image_utils import make_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Face Unlock\n",
        "\n",
        "Let's train a model to detect our face. We can think of this as a simpler version of one of the components inside something like the face ID software on our phones.\n",
        "\n",
        "We'll skip the face detection part, which is when we find faces in an image, and assume we can get cropped and aligned faces out of images or video streams. We'll look at face detection later in the semester.\n",
        "\n",
        "This is a slightly different kind of problem from the classification exercise we did in class, but the process is mostly the same.\n",
        "\n",
        "We will use a dataset with other people's faces, but in the end we are only interested on how well our model detects our face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### We Always Start with the Data\n",
        "\n",
        "The dataset we're using is inside `./data/images/lfw/cropped`. It's a subset of the [Labeled Faces in the Wild](https://vis-www.cs.umass.edu/lfw/) dataset.\n",
        "\n",
        "Take a look at the directory.\n",
        "\n",
        "What's there?\n",
        "\n",
        "How's the data organized and labeled?\n",
        "\n",
        "### Loading the Data\n",
        "\n",
        "Since we're not interested in generic classification, and measuring how we do on unlabeled data, this whole dataset is labeled, and we can read it into `train` and `test` subsets by calling the `train_test_split()` function of the `LFWUtils` class.\n",
        "\n",
        "This function takes an optional parameter that specifies what portion of the data should be used for the `test` dataset. We can start with the default value of $0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train, test = LFWUtils.train_test_split(dir=\"./data/image/lfw/cropped\", test_pct=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Looking at the Data\n",
        "\n",
        "Ok. Data is loaded.\n",
        "\n",
        "What's in the data? How is it actually organized?\n",
        "\n",
        "Take a look at the objects that were returned in each of the $2$ variables.\n",
        "\n",
        "How big are our datasets?\n",
        "\n",
        "Take a look at the `LABELS` and `L2I` members of the `LFWUtils` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: look at dataset objects (train and test variables). What's in them?\n",
        "# TODO: how big are them? (how many records?)\n",
        "# TODO: how many labels do they have?\n",
        "# TODO: what are the labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Data\n",
        "\n",
        "We can open some random images to make sure the content of our datasets make sense:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = len(train[\"labels\"])\n",
        "ridx = randint(0, train_size - 1)\n",
        "\n",
        "label_id = train[\"labels\"][ridx]\n",
        "\n",
        "print(\"id:\", label_id,\n",
        "      \"\\nlabel:\", LFWUtils.LABELS[label_id],\n",
        "      \"\\nfrom:\", train[\"files\"][ridx])\n",
        "\n",
        "display(make_image(train[\"pixels\"][ridx], width=LFWUtils.IMAGE_SIZE[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Adding your images\n",
        "\n",
        "Create a directory in the `dataset` directory for your images. Give it a one-word name, like your last name, your New School id or your initials. For example, mine is called `tgh` and is located at: `./data/images/lfw/cropped/tgh`.\n",
        "\n",
        "Now, add between $20$ and $30$ images of your face to your directory. \n",
        "\n",
        "The images should be just like the ones that are already there for the other people:\n",
        "- $130$ pixels wide by \n",
        "- $170$ pixels tall\n",
        "- single-channel grayscale\n",
        "- jpeg format\n",
        "- named `label-number.jpg` (for example: `tgh-000.jpg`)\n",
        "\n",
        "Feel free to do this manually using Photoshop or any other image editing software, but the easiest way is to use this interface that automatically crops faces out of pictures and creates images in the correct format:\n",
        "\n",
        "[Face Align](https://huggingface.co/spaces/visualizedata/PSAM5020-FaceAlign-Gradio)\n",
        "\n",
        "It will also align the faces and put the eyes in a consistent location. There's even an option to capture pictures from a live camera stream.\n",
        "\n",
        "### Reload Dataset\n",
        "\n",
        "Just run the `train_test_split()` again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PCA, Classification, etc etc etc\n",
        "\n",
        "Now that we have added our images to the dataset, let's train a classifier and see how well it performs on not just classification, but on recognizing our face.\n",
        "\n",
        "We can aim for an explained variance value of about $80\\%$, and adjust that later if we find necessary.\n",
        "\n",
        "Once we have the PCs for our training dataset in a `DataFrame` we can add a `label` column to it with the correct labels we have in `train[\"labels\"]`.\n",
        "\n",
        "We can also create a `DataFrame` for testing now by using the same `PCA` object to `transform()` the `test[\"pixels\"]` data.\n",
        "\n",
        "Since we won't train anything with the test dataset, it's ok to just keep the labels in `test[\"labels\"]` as they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: create PCA, fit and transform train data\n",
        "# TODO: check PCA captured variance\n",
        "# TODO: prepare DataFrame for training (add label column)\n",
        "# TODO: create the test DataFrame by running PCA on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the following cell to take a look at our images and their reconstructions.\n",
        "\n",
        "This assumes the `DataFrame` is called `train_df` and the `PCA` object is called `face_pca`. Adjust these if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_size = len(train[\"labels\"])\n",
        "ridx = randint(0, train_size - 1)\n",
        "\n",
        "# reconstruct image\n",
        "pca_pixels = face_pca.inverse_transform(train_df.iloc[[ridx]].drop(columns=[\"label\"]))\n",
        "\n",
        "display(make_image(train[\"pixels\"][ridx], width=LFWUtils.IMAGE_SIZE[0]))\n",
        "display(make_image(pca_pixels, width=LFWUtils.IMAGE_SIZE[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# filter the DataFrame by our label\n",
        "awesome_df = train_df[train_df[\"label\"] == LFWUtils.L2I[\"watts\"]]\n",
        "\n",
        "# save index of first image with our label\n",
        "awesome_idx = awesome_df.index[0]\n",
        "\n",
        "# reconstruct image\n",
        "pca_pixels = face_pca.inverse_transform(awesome_df.iloc[[0]].drop(columns=[\"label\"]))\n",
        "\n",
        "display(make_image(train[\"pixels\"][awesome_idx], width=LFWUtils.IMAGE_SIZE[0]))\n",
        "display(make_image(pca_pixels, width=LFWUtils.IMAGE_SIZE[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "<span style=\"color:hotpink;\">\n",
        "Do these make sense ? Do they look \"recognizable\" ? How do they change as a function of <code>n_components</code> ?\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, back to classifying...\n",
        "\n",
        "Maybe start with `RandomForestClassifier()` or `SVC()`..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "work_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# TODO: create a classifier\n",
        "# TODO: separate input and output columns from the train DataFrame\n",
        "# TODO: train model using train data and labels\n",
        "# TODO: run prediction on train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate model with training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# measure classification error\n",
        "print(\"error:\", classification_error(train[\"labels\"], train_predictions))\n",
        "\n",
        "# look at precision/recall from classification_report\n",
        "print(classification_report(train[\"labels\"], train_predictions))\n",
        "\n",
        "# look at confusion matrix\n",
        "display_confusion_matrix(train[\"labels\"], train_predictions, LFWUtils.LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "<span style=\"color:hotpink;\">\n",
        "How does the confusion matrix look ? What does it mean ?\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate model with testing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: run prediction on test data\n",
        "# TODO: measure classification error\n",
        "# TODO: look at precision/recall from classification_report\n",
        "# TODO: look at confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Different Classifiers\n",
        "\n",
        "Try changing the classifier type above, or some of its parameters, to see if the overall accuracy can be improved.\n",
        "\n",
        "Some things to try:\n",
        "- `SVC(kernel=\"linear\")`: this changes the type of curve the model tries to use to separate our classes. The more complex default type (`rbf`) might be over-fitting the data. We can experiment with `linear` or `poly` curves.\n",
        "- `LogisticRegression()`: the classification that fits statistical modeling functions (bell curves) to our data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyper-parameter Optimization\n",
        "\n",
        "Each model type has a handful of parameters that can be adjusted and optimized.\n",
        "\n",
        "For example, we could add \"regularization\" to a `LogisticRegression` classifier with `C=0.1`.\n",
        "\n",
        "Regularization is a process that makes training harder by setting more restrictions on the kind of answers it can give.\n",
        "\n",
        "This can be restrictions like:\n",
        "- keep all calculated parameters close to $0$\n",
        "- minimize the number of non-zero parameters used\n",
        "- keep all parameters positive\n",
        "\n",
        "The result is slightly worse performance on the training dataset, but hopefully better generalization of the model and better performance on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a classifier\n",
        "model = LogisticRegression(solver=\"liblinear\", C=0.1, random_state=1010)\n",
        "\n",
        "# train model using train data and labels\n",
        "model.fit(train_features, train_df[\"label\"])\n",
        "\n",
        "# run prediction\n",
        "train_predictions = model.predict(train_features)\n",
        "test_predictions = model.predict(test_df)\n",
        "\n",
        "# measure classification error\n",
        "print(\"error:\", classification_error(train[\"labels\"], train_predictions))\n",
        "print(\"error:\", classification_error(test[\"labels\"], test_predictions))\n",
        "\n",
        "# look at precision/recall and confusion matrix\n",
        "print(classification_report(test[\"labels\"], test_predictions))\n",
        "display_confusion_matrix(test[\"labels\"], test_predictions, LFWUtils.LABELS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "<span style=\"color:hotpink;\">\n",
        "How does THIS confusion matrix look ? What does it mean ? How does it perform for your pictures ?\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision and Recall\n",
        "\n",
        "Accuracy, which is the complement of our `classification_error` value, is the measurement that is optimized during the `RandomForestClassifier` training process.\n",
        "\n",
        "If we were training a regular classifier, we would look at `accuracy` (or `classification_error`) to determine if our model's performance is acceptable.\n",
        "\n",
        "Since we're working on a personal face recognition model, we don't really care about overall accuracy, but instead are more interested in the `precision` and `recall` values for the classification of our particular images.\n",
        "\n",
        "We don't want overall accuracy to be horrible, but we can be more specific in this case and be happy if the correct portion of our confusion matrix looks good.\n",
        "\n",
        "Calculate the `precision` and `recall` values for the classification of your images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "work_cell"
        ]
      },
      "outputs": [],
      "source": [
        "# TODO: calculate precision\n",
        "# TODO: calculate recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "<span style=\"color:hotpink;\">\n",
        "How is it performing for your images ? Which value, precision or recall, is higher ? What does that mean ?\n",
        "</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run the following cell to see which classes have the highest `precision` and `recall` scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"top precision:\", LFWUtils.top_precision(test[\"labels\"], test_predictions, top=5))\n",
        "print(\"top recall:\", LFWUtils.top_recall(test[\"labels\"], test_predictions, top=5))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "5020",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
