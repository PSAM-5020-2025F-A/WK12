{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12\n",
    "\n",
    "Tensors and Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import Adagrad, Adam, SGD\n",
    "\n",
    "from data_utils import object_from_json_url, regression_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price Regression\n",
    "\n",
    "Let's load the housing prices dataset from `HW03`.\n",
    "\n",
    "As always, we'll encode and scale our data if needed, and then we'll use the `train_test_split()` function to split our `DataFrame` into $2$ separate datasets, a training dataset with $80\\%$ of the rows, and a test dataset with $20\\%$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "HOUSES_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025F-A/5020-utils/main/datasets/json/LA_housing.json\"\n",
    "houses_info = object_from_json_url(HOUSES_FILE)\n",
    "\n",
    "# DataFrame it\n",
    "houses_raw_df = pd.DataFrame.from_records(houses_info)\n",
    "\n",
    "# Scale it\n",
    "# Note: (technically we should split, then scale, but this saves a few lines of code)\n",
    "house_scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "houses_df = house_scaler.fit_transform(houses_raw_df)\n",
    "\n",
    "# Keep original outcome variable values\n",
    "houses_df[\"value\"] = houses_raw_df[\"value\"]\n",
    "\n",
    "# Train/Test split\n",
    "houses_train_df, houses_test_df = train_test_split(houses_df, test_size=0.2, random_state=1010)\n",
    "\n",
    "houses_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic Linear Regression\n",
    "\n",
    "Let's set up a `LinearRegression()` model to predict house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat = houses_train_df.drop(columns=[\"value\"]).values\n",
    "train_out = houses_train_df[[\"value\"]].values\n",
    "\n",
    "test_feat = houses_test_df.drop(columns=[\"value\"]).values\n",
    "test_out = houses_test_df[[\"value\"]].values\n",
    "\n",
    "model = LinearRegression().fit(train_feat, train_out)\n",
    "\n",
    "train_pred = model.predict(train_feat)\n",
    "test_pred = model.predict(test_feat)\n",
    "\n",
    "print(\"train error\", regression_error(train_out, train_pred))\n",
    "print(\"test error\", regression_error(test_out, test_pred))\n",
    "\n",
    "plt.plot(sorted(train_out), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(train_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sorted(test_out), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(test_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model is off by almost a million dollars on average. We can add `PolynomialFeatures` to see how much more we can improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add Polynomial features\n",
    "\n",
    "poly = PolynomialFeatures(degree=3).set_output(transform=\"pandas\")\n",
    "\n",
    "train_poly_feat = poly.fit_transform(houses_train_df.drop(columns=[\"value\"])).values\n",
    "train_out = houses_train_df[[\"value\"]].values\n",
    "\n",
    "test_poly_feat = poly.transform(houses_test_df.drop(columns=[\"value\"])).values\n",
    "test_out = houses_test_df[[\"value\"]].values\n",
    "\n",
    "model = LinearRegression().fit(train_poly_feat, train_out)\n",
    "\n",
    "train_pred = model.predict(train_poly_feat)\n",
    "test_pred = model.predict(test_poly_feat)\n",
    "\n",
    "print(\"train error\", regression_error(train_out, train_pred))\n",
    "print(\"test error\", regression_error(test_out, test_pred))\n",
    "\n",
    "plt.plot(sorted(train_out), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(train_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sorted(test_out), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(test_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Linear Regression\n",
    "\n",
    "Just like with the `LinearRegression` model, we have to separate our independent features and our outcome feature.\n",
    "\n",
    "But, before we can put them through a neural network, we have to wrap them into `Tensor` objects.\n",
    "\n",
    "The `Tensor` objects are fancy lists that have some of the same functionality as our `DataFrames`, but have been optimized for neural network operations.\n",
    "\n",
    "We'll start by using the $5$ original features from our dataset, without any `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Tensor(train_feat)\n",
    "y_train = Tensor(train_out)\n",
    "\n",
    "x_test = Tensor(test_feat)\n",
    "y_test = Tensor(test_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our model architecture\n",
    "\n",
    "Instead of using a pre-build model algorithm, we'll create one from scratch, using neurons.\n",
    "\n",
    "We'll start with a very basic neural network model that has an input layer with a neuron for each feature, and a single output neuron for the price prediction.\n",
    "\n",
    "Something like this:\n",
    "\n",
    "<img src=\"./imgs/linear_5x1.jpg\" width=\"800px\"/>\n",
    "\n",
    "Where the initial values for the model parameters are selected at random by default.\n",
    "\n",
    "This is how we create this network using the `PyTorch` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(x_train.shape[1], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following function to iterate over out model's layers, get their parameters and print their shapes, or calculate overall number of parameters using the `numel()` function of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(m):\n",
    "  psum = 0\n",
    "  for p in m.parameters():\n",
    "    print(p.shape)\n",
    "    psum += p.numel()\n",
    "  return psum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of parameters:\", get_num_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model architecture\n",
    "\n",
    "We can run this model on our train dataset just to make sure all of our layers have the correct shapes and data is able to flow from the input to the output of our model.\n",
    "\n",
    "If anything is off we'll get an error here.\n",
    "\n",
    "We're giving our model a `Tensor` with $4623$ houses and $5$ features for each house. It should give us $4623$ predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model(x_train)\n",
    "\n",
    "print(\"shape of input data:\", x_train.shape)\n",
    "print(\"shape of output data:\", y_train.shape)\n",
    "print(\"shape of model output:\", y_train_pred.shape)\n",
    "\n",
    "print(\"first outcome value\", y_train[0].item())\n",
    "print(\"first predicted value\", y_train_pred[0].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training\n",
    "\n",
    "These are the `PyTorch` objects that we need to train our model:\n",
    "\n",
    "- `model`: our network, made up of neurons and their parameters\n",
    "- `optim`: optimizer that will adjust our model parameters\n",
    "- `loss_fn`: function to compute overall error of our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error Loss\n",
    "\n",
    "The cost of being wrong for a regression network is calculated using the *Mean Square Error* of our predictions. This is (yet) a different name for something that's pretty much the same as the *L2*/*Euclidean* distance between all predictions and their true values in the dataset. We've been using something like this with different names throughout the semester.\n",
    "\n",
    "Let's say we are training a regression neural network on a dataset with $6$ records/items. The correct and predicted values of our outcome variable are:\n",
    "\n",
    "| $\\quad\\text{true}\\quad$ | $\\quad\\text{predicted}\\quad$ |\n",
    "|:----:|:----:|\n",
    "| $1.06$ | $0.88$ |\n",
    "| $0.86$ | $0.72$ |\n",
    "| $2.98$ | $2.51$ |\n",
    "| $1.65$ | $1.74$ |\n",
    "| $0.92$ | $1.07$ |\n",
    "| $2.22$ | $1.99$ |\n",
    "\n",
    "The *Mean Square Error* is simply the square root of the sum of the individual differences squared:\n",
    "\n",
    "$$\\displaystyle\\text{MSE} = \\frac{(0.88 - 1.06)^2 + (0.72 - 0.86)^2 + (2.51 - 2.98)^2 + (1.74 - 1.65)^2 + (1.07 - 0.92)^2 + (1.99 - 2.22)^2}{6}$$\n",
    "\n",
    "This gives a single error value for all of our predictions, but since this math is being done with `Tensor` objects, the result can be back-propagated through the network to determine each neuron's contribution to the error.\n",
    "\n",
    "We'll use the built in [`MSELoss`](https://docs.pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) function from the `PyTorch` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(x_train.shape[1], 1)\n",
    "optim = SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network\n",
    "\n",
    "Training our neural network involves running the following steps iteratively until it stops improving:\n",
    "\n",
    "- calculate price predictions for all of the rows in our dataset\n",
    "- calculate the overall error/cost for the price predictions\n",
    "- annotate individual neuron contributions to the overall error\n",
    "- use annotations to update model parameters and decrease error\n",
    "\n",
    "A few things to note about this process:\n",
    "\n",
    "1\\. We are calculating all of the predictions for all of our data with a single call: `y = model(x)`. `PyTorch` models are smart and they know we want to do the same thing for all of the rows in our data. This optimizes and parallelizes the process.\n",
    "\n",
    "2\\. The cost/error of our predictions (called `loss` here) is the mean squared error between all price predictions and all actual prices in our dataset, calculated in one go using the builtin [`MSELoss`](https://docs.pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) function from the `PyTorch` library. This function has a method called `backward()` that will back-propagate the error and annotate each individual neuron's contribution to the overall error.\n",
    "\n",
    "3\\. The optimizer will use these annotated values to optimize and update the weights and thresholds of each of our $6$ neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate 32 times\n",
    "for c in range(32):\n",
    "  # reset neuron annotations\n",
    "  optim.zero_grad()\n",
    "\n",
    "  # get predictions\n",
    "  y_pred = model(x_train)\n",
    "\n",
    "  # get iteration error\n",
    "  loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "  # annotate neurons\n",
    "  loss.backward()\n",
    "\n",
    "  # update parameters\n",
    "  optim.step()\n",
    "\n",
    "  # print error\n",
    "  if c % 4 == 0:\n",
    "    print(f\"  {c}\"[-2:], \"loss:\", loss.to(int).item(), \"| avg error:\", loss.pow(0.5).to(int).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:32px\">ðŸ¤”</span>\n",
    "\n",
    "What's happening in the above cell?\n",
    "\n",
    "What happens if we keep running it over and over?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the train dataset\n",
    "\n",
    "Once the training settles, we can calculate predictions for both train and test datasets and see how our model performs.\n",
    "\n",
    "The error/loss value will decrease during training, but eventually it will settle and our trained model will be about as good as the classic `LinearRegression` model above.\n",
    "\n",
    "Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model(x_train).tolist()\n",
    "y_test_pred = model(x_test).tolist()\n",
    "\n",
    "print(\"train error\", regression_error(y_train, y_train_pred))\n",
    "print(\"test error\", regression_error(y_test, y_test_pred))\n",
    "\n",
    "plt.plot(sorted(train_out), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(y_train_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sorted(test_out), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(y_test_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Output\n",
    "\n",
    "Before we go much further with neural networks we should scale our outcome variables.\n",
    "\n",
    "Due to the nature of the activation functions, it helps our network if all of our values are well-bounded and kept close to $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale output feature\n",
    "\n",
    "out_scaler = StandardScaler()\n",
    "\n",
    "train_out_std = out_scaler.fit_transform(houses_train_df[[\"value\"]])\n",
    "test_out_std = out_scaler.transform(houses_test_df[[\"value\"]])\n",
    "\n",
    "x_train = Tensor(train_feat)\n",
    "y_train = Tensor(train_out_std)\n",
    "\n",
    "x_test = Tensor(test_feat)\n",
    "y_test = Tensor(test_out_std)\n",
    "\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing the Network\n",
    "\n",
    "The error we were getting above was still around a million dollars. That's less than $1.0$ standard deviations... it's not bad, but it's also not good.\n",
    "\n",
    "If we want to improve our model we can try adding layers to our Neural Network. We just have to make sure we add an activation function between the neurons. These are the functions that keep our model parameters within a nice, well-defined, range and, more importantly, allow the model to learn non-linear relationships between input and output features.\n",
    "\n",
    "We're going to build the following network:\n",
    "\n",
    "<img src=\"./imgs/linear_5x5x1.jpg\" width=\"800px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "  nn.Linear(x_train.shape[1], x_train.shape[1]),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Linear(x_train.shape[1], 1),\n",
    ")\n",
    "\n",
    "optim = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# display the number of parameters\n",
    "print(\"number of parameters:\", get_num_params(model))\n",
    "\n",
    "# run on train data and check shape of output\n",
    "y_pred = model(x_train)\n",
    "\n",
    "x_train.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about our optimizer\n",
    "\n",
    "We're using one of the simpler optimizers in `PyTorch` to perform [_stochastic gradient descent_](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Gradient descent is the official name of the algorithm that determines whether parameters have to increase or decrease, and by how much. _Stochastic_ means that it should still work if we sub-sample our input data and only use a subset of the data points at a time. It remembers/accumulates information about previous error measurements.\n",
    "\n",
    "The documentation for the [`SGD` optimizer](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) has more info about the algorithm and the parameters it takes.\n",
    "\n",
    "Other than simplifying our training code, these pre-built optimizers also perform dynamic learning rate adjustment and some other tricks that make our overall process not so sensitive to an exact learning rate.\n",
    "\n",
    "The `PyTorch` library also has a number of [other optimizers](https://pytorch.org/docs/stable/optim.html#algorithms) useful for performing gradient descent. In addition to `SGD` we can also try [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) or [Adagrad](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it\n",
    "\n",
    "We can train our new model, just like before, but since we have more parameters, it might take a bit longer to settle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate 1000 times\n",
    "for c in range(1000):\n",
    "  # reset neuron annotations\n",
    "  optim.zero_grad()\n",
    "  # get predictions\n",
    "  y_pred = model(x_train)\n",
    "  # get iteration error\n",
    "  loss = loss_fn(y_pred, y_train)\n",
    "  # annotate neurons\n",
    "  loss.backward()\n",
    "  # update parameters\n",
    "  optim.step()\n",
    "\n",
    "  if c % 250 == 0:\n",
    "    print(c, loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset\n",
    "\n",
    "We can still adjust a lot of parameters here, but before we spend too much time on this model, let's run it on the test dataset and calculate the average loss on data that wasn't used for training to see if the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model(x_train).tolist()\n",
    "y_test_pred = model(x_test).tolist()\n",
    "\n",
    "print(\"train error\", regression_error(y_train, y_train_pred))\n",
    "print(\"test error\", regression_error(y_test, y_test_pred))\n",
    "\n",
    "plt.plot(sorted(train_out_std), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(y_train_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sorted(test_out_std), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(y_test_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model(x_train).tolist()\n",
    "y_test_pred = model(x_test).tolist()\n",
    "\n",
    "# TODO: un-scale output feature\n",
    "\n",
    "dollar_train = out_scaler.inverse_transform(y_train)\n",
    "dollar_train_pred = out_scaler.inverse_transform(y_train_pred)\n",
    "\n",
    "dollar_test = out_scaler.inverse_transform(y_test)\n",
    "dollar_test_pred = out_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "print(\"train error\", regression_error(dollar_train, dollar_train_pred))\n",
    "print(\"test error\", regression_error(dollar_test, dollar_test_pred))\n",
    "\n",
    "plt.plot(sorted(dollar_train), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(dollar_train_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sorted(dollar_test), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(dollar_test_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "This isn't bad.\n",
    "\n",
    "The absolute value of the error is kind of large, but the test dataset error is comparable to the training dataset error, which is a good indication that the model is not over-fitting.\n",
    "\n",
    "Also, the plots show that the model is doing better at being able to achieve the price values in the dataset.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "We can spend some time adjusting the model, adding layers, changing the optimizer, the learning rate, experimenting with the optimizer's parameters, etc.\n",
    "\n",
    "This process is usually referred to as _hyperparameter_ tuning, since we're picking parameters that will help us calculate the parameters of our neural network.\n",
    "\n",
    "Here's a cell with all of the steps combined. We can play with the network architecture and parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "x_train = Tensor(train_feat)\n",
    "y_train = Tensor(train_out_std)\n",
    "x_test = Tensor(test_feat)\n",
    "y_test = Tensor(test_out_std)\n",
    "\n",
    "\n",
    "# TODO: adjust parameters, add layers\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(x_train.shape[1], 16 * x_train.shape[1]),\n",
    "  nn.ReLU(),\n",
    "\n",
    "  nn.Linear(16 * x_train.shape[1], 1),\n",
    ")\n",
    "\n",
    "# print the number of parameters\n",
    "print(\"number of parameters:\", get_num_params(model))\n",
    "\n",
    "\n",
    "# TODO: adjust parameters, add parameters, change optimizer\n",
    "optim = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Train Model\n",
    "for c in range(1440):\n",
    "  optim.zero_grad()\n",
    "  y_pred = model(x_train)\n",
    "  loss = loss_fn(y_pred, y_train)\n",
    "  loss.backward()\n",
    "  optim.step()\n",
    "\n",
    "  if c % 128 == 0:\n",
    "    print(c, loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "y_train_pred = model(x_train).tolist()\n",
    "y_test_pred = model(x_test).tolist()\n",
    "\n",
    "print(\"train error\", regression_error(y_train, y_train_pred))\n",
    "print(\"test error\", regression_error(y_test, y_test_pred))\n",
    "\n",
    "# TODO: un-scale output feature\n",
    "\n",
    "dollar_train = out_scaler.inverse_transform(y_train)\n",
    "dollar_train_pred = out_scaler.inverse_transform(y_train_pred)\n",
    "\n",
    "dollar_test = out_scaler.inverse_transform(y_test)\n",
    "dollar_test_pred = out_scaler.inverse_transform(y_test_pred)\n",
    "\n",
    "print(\"train error\", regression_error(dollar_train, dollar_train_pred))\n",
    "print(\"test error\", regression_error(dollar_test, dollar_test_pred))\n",
    "\n",
    "plt.plot(sorted(dollar_train), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(dollar_train_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(sorted(dollar_test), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8)\n",
    "plt.plot(sorted(dollar_test_pred), linestyle=\"\", marker=\"o\", markersize=2, alpha=0.8, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Our model is definitely learning, and seems like it's able to learn non-linear relationships like `PolynomialFeatures` automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gradio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
